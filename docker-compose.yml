services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-qwen3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    ports:
      - "8000:8000"
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_HOME=/models/huggingface
      - TRANSFORMERS_CACHE=/models/huggingface
      - HF_HUB_CACHE=/models/huggingface
    
    command: >
      Qwen/Qwen3-30B-A3B
      --host 0.0.0.0
      --port 8000
      --dtype auto
      --gpu-memory-utilization 0.90
      --max-model-len 32768
      --reasoning-parser qwen3
      --enable-auto-tool-choice
      --tool-call-parser hermes
    
    volumes:
      - /app/models:/models
    
    restart: unless-stopped

  dcgm-exporter:
    image: nvcr.io/nvidia/k8s/dcgm-exporter:4.4.2-4.7.1-ubuntu22.04
    container_name: dcgm_exporter
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    cap_add:
      - SYS_ADMIN
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "9401:9400"
    restart: unless-stopped

  node_exporter:
    image: quay.io/prometheus/node-exporter:latest
    container_name: node_exporter
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    ports:
      - "9100:9100"
    volumes:
      - '/:/host:ro,rslave'
