services:
  vllm-gpu1:
    image: vicharanashala/my-vllm-patched:v0.10.2
    container_name: vllm-gpt-oss-gpu1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia

              device_ids: ['0']
              capabilities: [gpu]
    
    ports:
      - "8001:8000"
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - HF_HOME=/models/huggingface
      - TRANSFORMERS_CACHE=/models/huggingface
      - HF_HUB_CACHE=/models/huggingface
    
    command: >
      --host 0.0.0.0
      --model openai/gpt-oss-120b
      --kv-cache-dtype fp8
      --max-model-len 32768
      --gpu-memory-utilization 0.95
      --enable-auto-tool-choice
      --tool-call-parser openai
      --reasoning-parser openai_gptoss
    
    volumes:
      - /app/models:/models
    
    restart: unless-stopped

  vllm-gpu2:
    image: vicharanashala/my-vllm-patched:v0.10.2
    container_name: vllm-gpt-oss-gpu2
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia

              device_ids: ['1']
              capabilities: [gpu]
    
    ports:
      - "8002:8000"
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
      - HF_HOME=/models/huggingface
      - TRANSFORMERS_CACHE=/models/huggingface
      - HF_HUB_CACHE=/models/huggingface
    
    command: >
      --host 0.0.0.0
      --model openai/gpt-oss-120b
      --kv-cache-dtype fp8
      --max-model-len 32768
      --gpu-memory-utilization 0.95
      --enable-auto-tool-choice
      --tool-call-parser openai
      --reasoning-parser openai_gptoss
    
    volumes:
      - /app/models:/models
    
    restart: unless-stopped 

  vllm-gpu3:
    image: vicharanashala/my-vllm-patched:v0.10.2
    container_name: vllm-gpt-oss-gpu3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia

              device_ids: ['2']
              capabilities: [gpu]
    
    ports:
      - "8003:8000"
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=2
      - HF_HOME=/models/huggingface
      - TRANSFORMERS_CACHE=/models/huggingface
      - HF_HUB_CACHE=/models/huggingface
    
    command: >
      --host 0.0.0.0
      --model openai/gpt-oss-120b
      --kv-cache-dtype fp8
      --max-model-len 32768
      --gpu-memory-utilization 0.95
      --enable-auto-tool-choice
      --tool-call-parser openai
      --reasoning-parser openai_gptoss
    
    volumes:
      - /app/models:/models
    
    restart: unless-stopped
    


  dcgm-exporter:
    image: nvcr.io/nvidia/k8s/dcgm-exporter:4.4.2-4.7.1-ubuntu22.04
    container_name: dcgm_exporter
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    cap_add:
      - SYS_ADMIN
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "9401:9400"
    restart: unless-stopped

  node_exporter:
    image: quay.io/prometheus/node-exporter:latest
    container_name: node_exporter
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    ports:
      - "9100:9100"
    volumes:
      - '/:/host:ro,rslave'
